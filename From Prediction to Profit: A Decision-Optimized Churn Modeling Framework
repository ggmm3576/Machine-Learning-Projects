---
title: "From Prediction to Profit: A Decision-Optimized Churn Modeling Framework (Fall)"
author: |
  **Gaurav Mishra**  
  Team: <Gaurav Mishra>
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    df_print: paged
    code_folding: show
fontsize: 11pt
---


```{r, echo=FALSE,eval=TRUE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE)

# Print environment summary
sessionInfo()

```


# 1. Business problem summary

Goal:- Estimating a calibrated probability of churn for each active customer so managers can target retention offers within a fixed budget.

1. A reliable score for each active customer that estimates the chance they will cancel in the next period.
2. A simple, budget aware rule that tells the retention team whom to contact and how many to contact per 1,000 customers.

Targeted outreach reduces revenue loss and marketing waste. A probability score lets the business pick a single threshold that fits monthly contact capacity and expected ROI.


Data at hand. train.csv with customer demographics, services, contract/billing info, charges, tenure, and target Churn (Yes/No).



# 2.  Leakage Policy

Scoring assumption:

Customers are scored at the end of a billing cycle to plan retention actions for the next 30 days. All variables represent information known before the churn decision window.

Leakage prevention measures implemented in this project:
	1.	Temporal data discipline
	•	All predictors (e.g., tenure, MonthlyCharges, TotalCharges) reflect customer status at the time of scoring.
	•	No future or post-churn events (like reactivations or support contacts) were included.
	
	2.	Target isolation
	•	The target variable Churn was never used to derive any feature.
	•	All transformations (like log or ratio features) were based only on non-target numeric columns.
	
	3.	Controlled preprocessing and encoding
	•	Data was split into training and testing sets before any transformation.
	•	Factor levels were aligned using the align_levels() function, ensuring encodings were learned from training data only and reused on test data.
	•	No scaling, imputation, or encoding used information from test rows.
	
	4.	Cross-validation without leakage
	•	Stratified 5-fold cross-validation was used to create foldid, ensuring the same folds for all models.
	•	Each fold’s validation set was strictly unseen during training.
	•	Out-of-fold (OOF) predictions were generated honestly, simulating unseen data.
	
	5.	Feature engineering safeguards
	•	Engineered features like log_MonthlyCharges, log_TotalCharges, charge_per_month, and tenure_band were derived strictly from existing variables available before churn.
	•	“No internet service” and similar flags were retained as valid categories rather than removed or inferred.
	
	6.	Calibration integrity
	•	Platt calibration models were trained only on OOF predictions (never on the same data used to train the base model).
	•	The calibrated probabilities were then applied to the test set, not refit.
	
	7.	Holdout file readiness
	•	A frozen pipeline (final_elasticnet_logistic_pipeline.rds) ensures consistent transformations and predictions.
	•	The holdout dataset, when released, will be scored once without re-fitting, preserving temporal and procedural integrity.

Summary:

All preprocessing, feature creation, model fitting, and calibration were designed to reflect only the data available at the time of scoring. No future information, target-dependent transformations, or test data statistics were used—fully preventing both target leakage and train–test contamination.
	
	
	
# 3 . Loading Libraries


Loading all necessary R packages for data manipulation, visualization, and modeling
This code imports the full tidyverse and tidymodels ecosystem, plus caret, xgboost, ranger, and other helper libraries used throughout the churn prediction project.
It also sets a fixed random seed (1234) for reproducibility.
	
	
```{r, Loading Library}
# ============================================================
# Load Libraries and Setup
# ============================================================

# Core tidyverse suite: data handling + visualization
library(tidyverse)      # includes ggplot2, dplyr, tidyr, readr, stringr, forcats

# ggplot2 for detailed, layered graphics
library(ggplot2)

# gt for clean and formatted tables in reports
library(gt)

# Tidymodels ecosystem: unified ML framework
library(tidymodels)     # parsnip, recipes, workflows, rsample, yardstick, tune

# caret for additional modeling utilities, grid search, and confusion matrix
library(caret)

# ML engines
library(ranger)         # Random Forest
library(xgboost)        # Gradient Boosting
library(glmnet)         # Logistic Regression (regularized)

# Evaluation and calibration tools
library(pROC)           # ROC curves and AUC

library(probably)       # Platt scaling & calibration
library(yardstick)      # performance metrics (AUC, Brier, accuracy, etc.)

# Visualization helpers
library(vip)            # variable importance plots
library(cowplot)        # combine multiple ggplots
library(gridExtra)      # flexible plot layouts
library(ggcorrplot)     # correlation matrix visualization

# Data utilities
library(janitor)        # cleaning column names, quick summaries
library(lubridate)      # date/time handling
library(scales)         # number formatting in plots
library(themis)         # handle class imbalance (over/under-sampling)
library(rsample)        # train/test split and cross-validation sets

# Optional: convenience packages used in earlier assignments
if (requireNamespace("skimr", quietly = TRUE)) library(skimr)       # compact data summaries
if (requireNamespace("reshape2", quietly = TRUE)) library(reshape2) # reshaping data

# Reproducibility
set.seed(1234)



```
	
	

# 4 . Data Setup and Initial Inspection 



This section loads the churn dataset, inspects its structure, checks data types, and provides an overview using summary statistics to understand the variable composition before cleaning or modeling.


```{r}
# ============================================================
# Data Setup and Initial Inspection
# ============================================================

# Load the dataset
churn_data <- readr::read_csv("churn_train.csv",
                              show_col_types = FALSE) |>
  mutate(TotalCharges = suppressWarnings(as.numeric(TotalCharges)))

# Preview first few records
head(churn_data)

# Check structure: variable names, types, and sample data
str(churn_data)

# Quick summary statistics for all variables
summary(churn_data)

# Check dataset dimensions (rows × columns)
dim(churn_data)

# Display column names
colnames(churn_data)

# Count missing or blank values in each column
churn_data %>%
  summarise(across(everything(), ~ sum(is.na(.x) | .x == ""))) %>%
  pivot_longer(cols = everything(),
               names_to = "Variable",
               values_to = "Missing_Count")

# Optional: quick descriptive snapshot (if skimr package available)
if ("skimr" %in% installed.packages()) {
  skimr::skim(churn_data)
}
```


$*EDA*$


# 5 . Data Type Review and Factor Conversion


This section examines all variables, identifies which columns are character (categorical), converts them into factors for modeling, and checks row completeness (number of non-missing values per row).

```{r}
# ============================================================
# Data Type Review and Factor Conversion
# ============================================================


# View all column names and their data types
glimpse(churn_data)

# Identify character columns
char_vars <- churn_data %>%
  select(where(is.character)) %>%
  names()

# Print character columns (to confirm which ones will become factors)
char_vars

# Convert all character variables to factors
churn_data <- churn_data %>%
  mutate(across(all_of(char_vars), as.factor))

# Verify data types after conversion
str(churn_data)

# Check number of fully complete rows (no missing values)
complete_rows <- churn_data %>%
  mutate(full_complete = complete.cases(.)) %>%
  summarise(
    total_rows = n(),
    complete_rows = sum(full_complete),
    incomplete_rows = total_rows - complete_rows,
    completeness_percent = round((complete_rows / total_rows) * 100, 2)
  )

# Display completeness summary
gt(complete_rows)

# Optional: count missing values per variable again (after conversion)
churn_data %>%
  summarise(across(everything(), ~ sum(is.na(.x) | .x == ""))) %>%
  pivot_longer(cols = everything(),
               names_to = "Variable",
               values_to = "Missing_Count") %>%
  arrange(desc(Missing_Count)) %>%
  gt()
```


## . Diagnose the 8 incomplete rows

Inspect which rows are incomplete and confirm the cause (TotalCharges == NA).

```{r}
# ============================================================
# Diagnose the 8 incomplete rows
# ============================================================


incomplete <- churn_data %>%
  filter(!complete.cases(.))

incomplete %>%
  select(ID, tenure, MonthlyCharges, TotalCharges, Contract, InternetService) %>%
  print(n = Inf)
```


## . Fix Missing TotalCharges for New Customers



For customers with tenure == 0, TotalCharges should logically be 0, since they have just joined and have not been billed yet. This deterministic fix avoids unnecessary imputation and keeps data consistent for modeling.

```{r}
# ============================================================
# Fix Missing TotalCharges for New Customers
# ============================================================

# Replace NA TotalCharges with 0 where tenure == 0
churn_data <- churn_data %>%
  mutate(TotalCharges = if_else(is.na(TotalCharges) & tenure == 0, 0, TotalCharges))

# Verify that all missing values are fixed
churn_data %>%
  summarise(Missing_TotalCharges = sum(is.na(TotalCharges)))

# Confirm total completeness again
churn_data %>%
  summarise(
    total_rows = n(),
    complete_rows = sum(complete.cases(.)),
    incomplete_rows = n() - sum(complete.cases(.)),
    completeness_percent = round((sum(complete.cases(.)) / n()) * 100, 2)
  )

```


## .Post-Fix Validation


Re-verify that (a) TotalCharges has no NAs, (b) all rows are complete, and (c) overall missingness by column is zero.

```{r}
# ============================================================
# Post-Fix Validation
# ============================================================

# Re-verify that (a) TotalCharges has no NAs, (b) all rows are complete,
# and (c) overall missingness by column is zero.

# (a) TotalCharges NA check
sum(is.na(churn_data$TotalCharges))

# (b) Row completeness summary
churn_data %>%
  summarise(
    total_rows        = n(),
    complete_rows     = sum(complete.cases(.)),
    incomplete_rows   = n() - sum(complete.cases(.)),
    completeness_pct  = round(100 * complete_rows / total_rows, 2)
  )

# (c) Missing count per column (should be 0 across all)
miss_tbl <- churn_data %>%
  summarise(across(everything(), ~ sum(is.na(.x) | (.x == "" & is.character(.x))))) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  arrange(desc(Missing_Count))
gt(miss_tbl)

# Optional sanity: show tenure==0 rows now have TotalCharges==0
churn_data %>%
  filter(tenure == 0) %>%
  select(ID, tenure, MonthlyCharges, TotalCharges) %>%
  head(8)
```




## . Correlation & Target Visualization


In this place,We have inspected numeric correlations, visualized correlation structure by churn class, compared numeric predictors by churn, and examined churn proportions and rates across key categorical features.

```{r}

# ============================================================
# 5.x Correlation & Target Visualization
# ============================================================
# 1) Correlations among numeric predictors
# 2) Correlation structure by churn class
# 3) Numeric predictors vs Churn (boxplots)
# 4) Categorical predictors vs Churn (proportion bars)
# 5) Churn-rate tables for key categoricals

library(psych)  # for pairs.panels

# ---- 1) Numeric correlations: matrix + heatmap ----
numeric_vars <- c("tenure", "MonthlyCharges", "TotalCharges")

corr_mat <- churn_data %>%
  dplyr::select(dplyr::all_of(numeric_vars)) %>%
  stats::cor(use = "pairwise.complete.obs")

print(corr_mat)

ggcorrplot::ggcorrplot(corr_mat, lab = TRUE, type = "lower") +
  ggtitle("Correlation Heatmap: tenure, MonthlyCharges, TotalCharges")

# ---- 2) Correlation structure by churn class ----
num_yes <- churn_data %>% dplyr::filter(Churn == "Yes") %>% dplyr::select(dplyr::all_of(numeric_vars))
num_no  <- churn_data %>% dplyr::filter(Churn == "No")  %>% dplyr::select(dplyr::all_of(numeric_vars))

pairs.panels(num_no,  main = "Numeric Correlations | Churn = No")
pairs.panels(num_yes, main = "Numeric Correlations | Churn = Yes")

# ---- 3) Numeric predictors vs Churn (boxplots) ----
churn_data %>%
  tidyr::pivot_longer(cols = dplyr::all_of(numeric_vars),
                      names_to = "Variable", values_to = "Value") %>%
  ggplot2::ggplot(ggplot2::aes(x = Churn, y = Value)) +
  ggplot2::geom_boxplot() +
  ggplot2::facet_wrap(~ Variable, scales = "free_y", ncol = 3) +
  ggplot2::labs(title = "Numeric Predictors vs Churn", x = NULL, y = NULL)

# ---- 4) Categorical predictors: stacked proportions by Churn ----
cat_vars <- c("Contract","InternetService","PaymentMethod",
              "PaperlessBilling","MultipleLines")

plot_cat_prop <- function(var) {
  var_quo <- rlang::enquo(var)
  churn_data %>%
    dplyr::mutate(!!var_quo := forcats::fct_infreq(!!var_quo)) %>%
    ggplot2::ggplot(ggplot2::aes(x = !!var_quo, fill = Churn)) +
    ggplot2::geom_bar(position = "fill") +
    ggplot2::scale_y_continuous(labels = scales::percent_format()) +
    ggplot2::labs(title = paste("Churn Proportion by", rlang::as_name(var_quo)),
                  x = NULL, y = "Churn proportion") +
    ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 20, hjust = 1))
}

# Render key proportion plots
plot_cat_prop(Contract)
plot_cat_prop(InternetService)
plot_cat_prop(PaymentMethod)
plot_cat_prop(PaperlessBilling)
plot_cat_prop(MultipleLines)

# SeniorCitizen handled separately (convert to factor just for this plot)
churn_data %>%
  dplyr::mutate(SeniorCitizen = factor(SeniorCitizen, levels = c(0,1), labels = c("No","Yes"))) %>%
  ggplot2::ggplot(ggplot2::aes(x = SeniorCitizen, fill = Churn)) +
  ggplot2::geom_bar(position = "fill") +
  ggplot2::scale_y_continuous(labels = scales::percent_format()) +
  ggplot2::labs(title = "Churn Proportion by Senior Citizen",
                x = NULL, y = "Churn proportion")

# ---- 5) Robust churn-rate table helper (fixed NSE) ----
rate_tbl <- function(var) {
  var_quo <- rlang::enquo(var)
  label   <- rlang::as_name(var_quo)

  df <- churn_data %>%
    dplyr::group_by(!!var_quo) %>%
    dplyr::summarise(churn_rate = mean(Churn == "Yes"), .groups = "drop") %>%
    dplyr::arrange(dplyr::desc(churn_rate))

  df %>%
    gt::gt() %>%
    gt::fmt_percent(columns = churn_rate, decimals = 1) %>%
    gt::tab_header(title = paste("Churn Rate by", label))
}

# Example churn-rate tables
rate_tbl(Contract)
rate_tbl(InternetService)
rate_tbl(PaymentMethod)
rate_tbl(PaperlessBilling)
rate_tbl(MultipleLines)
```



### . Checking the significance level:-(This is only for checking the parameter)


### . Using effect sizes and AUC for a meaningful ranking

Purpose: rank variables by strength of association with Churn.


Categoricals — Cramér’s V (association strength)

So,
contract type, tech-support-related features, and payment/billing methods dominate churn behavior. Gender and phone service contribute nothing measurable.

Numerics — Cohen’s d (mean separation)

customers who just joined and pay high monthly rates are the most unstable.

Univariate AUC (predictive power of each variable alone)

1. Contract (0.74) – best single variable.
2.	tenure (0.74) – almost equal power.
3–5. OnlineSecurity, TechSupport, InternetService (≈ 0.70) – each adds meaningful signal.

Lower tiers (0.60-0.65): payment, add-ons, and billing method.

Below 0.55 – weak to none (demographics).


```{r}


# ============================================================
# Effect sizes (Cramér's V, Cohen's d) and Univariate AUC
# ============================================================


library(pROC)

# --- helper: Cramér's V from a contingency table (no bias arg) ---
cramers_v <- function(tbl) {
  chi2 <- suppressWarnings(chisq.test(tbl, correct = FALSE)$statistic)
  n    <- sum(tbl)
  r    <- nrow(tbl); k <- ncol(tbl)
  v    <- sqrt(as.numeric(chi2) / (n * min(k - 1, r - 1)))
  v
}

# Categorical vars (coerce SeniorCitizen to factor for association)
cat_vars <- c("gender","SeniorCitizen","Partner","Dependents","PhoneService",
              "MultipleLines","InternetService","OnlineSecurity","OnlineBackup",
              "DeviceProtection","TechSupport","StreamingTV","StreamingMovies",
              "Contract","PaperlessBilling","PaymentMethod")

cramers <- purrr::map_dfr(cat_vars, function(v){
  x <- churn_data[[v]]
  if (is.numeric(x)) x <- factor(x)                # e.g., SeniorCitizen 0/1
  tbl <- table(x, churn_data$Churn)
  tibble::tibble(Variable = v, Cramers_V = cramers_v(tbl))
}) %>% dplyr::arrange(dplyr::desc(Cramers_V))

# --- Numeric effect sizes: Cohen's d (absolute) ---
# install.packages("effsize") if missing
has_eff <- requireNamespace("effsize", quietly = TRUE)
if (has_eff) {
  # compute Cohen’s d
} else {
  message("Skipping Cohen's d: package 'effsize' not installed.")
}
num_vars <- c("tenure","MonthlyCharges","TotalCharges")
cohen <- purrr::map_dfr(num_vars, function(v){
  d <- effsize::cohen.d(churn_data[[v]] ~ churn_data$Churn, hedges.correction = TRUE)
  tibble::tibble(Variable = v, Cohens_d = abs(unname(d$estimate)))
}) %>% dplyr::arrange(dplyr::desc(Cohens_d))

# --- Univariate AUC for every predictor ---
all_preds <- setdiff(names(churn_data), c("ID","Churn"))
y <- ifelse(churn_data$Churn=="Yes",1,0)

uni_auc <- purrr::map_dfr(all_preds, function(v){
  f <- stats::as.formula(paste("Churn ~ `", v, "`", sep = ""))
  m <- try(stats::glm(f, data = churn_data, family = stats::binomial()), silent = TRUE)
  if (inherits(m, "try-error")) return(tibble::tibble(Variable=v, AUC=NA_real_))
  p <- suppressWarnings(stats::predict(m, type="response"))
  tibble::tibble(Variable = v, AUC = as.numeric(pROC::auc(y, p)))
}) %>% dplyr::arrange(dplyr::desc(AUC))

# --- Pretty print tables ---
gt::gt(cramers) %>% gt::fmt_number(columns = "Cramers_V", decimals = 3) %>%
  gt::tab_header(title = "Categoricals ranked by Cramér’s V")

gt::gt(cohen) %>% gt::fmt_number(columns = "Cohens_d", decimals = 3) %>%
  gt::tab_header(title = "Numerics ranked by Cohen’s d")

gt::gt(uni_auc) %>% gt::fmt_number(columns = "AUC", decimals = 3) %>%
  gt::tab_header(title = "Univariate AUC ranking")

```





# 6 . Feature Preparation and Preprocessing


Building a unified tidymodels recipe that cleans and encodes data consistently for all models (logistic, RF, XGB).


```{r}
# ============================================================
# Feature Preparation and Preprocessing
# ============================================================

# 6.1 Feature grouping (for ablation later)
features_demo     <- c("gender", "SeniorCitizen", "Partner", "Dependents")
features_service  <- c("PhoneService","MultipleLines","InternetService",
                       "OnlineSecurity","OnlineBackup","DeviceProtection",
                       "TechSupport","StreamingTV","StreamingMovies")
features_contract <- c("Contract","PaperlessBilling","PaymentMethod")
features_numeric  <- c("tenure","MonthlyCharges","TotalCharges")
target_var        <- "Churn"

# 6.2 Normalize “No service” categories
churn_data <- churn_data %>%
  mutate(across(all_of(c("MultipleLines","OnlineSecurity","OnlineBackup",
                         "DeviceProtection","TechSupport","StreamingTV",
                         "StreamingMovies")),
                ~ forcats::fct_recode(.x,
                   "No" = "No internet service",
                   "No" = "No phone service")))

# 6.3 Factor order: ensure 'No' before 'Yes'
churn_data <- churn_data %>%
  mutate(across(where(is.factor),
                ~ forcats::fct_relevel(.x, c("No","Yes"), after = 0)))

# 6.4 Check churn balance
table(churn_data$Churn)
prop.table(table(churn_data$Churn))

# 6.5 Split 80/20 stratified by Churn
set.seed(1234)
split_obj  <- rsample::initial_split(churn_data, prop = 0.8, strata = Churn)
train_data <- rsample::training(split_obj)
test_data  <- rsample::testing(split_obj)

# 6.6 Build preprocessing recipe
base_recipe <- recipes::recipe(Churn ~ ., data = train_data) %>%
  update_role(ID, new_role = "ID") %>%
  step_rm(ID) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_zv(all_predictors())

# 6.7 Preview recipe
summary(base_recipe)
```



## . Pre-Model Readiness Checklist


Run all preflight checks before Step 7 (baseline logistic). Verifies NA status, factor levels, numeric sanity, correlations, class balance, and recipe output integrity in one go.


```{r}
# ============================================================
# Pre-Model Readiness Checklist (single chunk)
# ============================================================


# --- 0) Objects expected to exist: churn_data, train_data, test_data, base_recipe ---

cat("\n==== 1) NA CHECKS ====\n")
na_total <- sum(is.na(train_data))
cat("Total NAs in train_data:", na_total, "\n")

na_by_col <- train_data %>%
  summarise(across(everything(), ~ sum(is.na(.x)))) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "NA_Count") %>%
  arrange(desc(NA_Count))
print(head(na_by_col, 10))

cat("\n==== 2) FACTOR LEVELS & TYPES ====\n")
cat("Churn levels:", paste(levels(train_data$Churn), collapse = ", "), "\n")
type_tbl <- tibble(
  Variable = names(train_data),
  Class    = purrr::map_chr(train_data, ~ class(.x)[1])
)
print(type_tbl)

cat("\n==== 3) NUMERIC SANITY (summary) ====\n")
num_vars <- c("tenure","MonthlyCharges","TotalCharges")
print(summary(train_data[, num_vars]))

cat("\n==== 4) NUMERIC CORRELATIONS ====\n")
corr_mat <- cor(train_data[, num_vars], use = "pairwise.complete.obs")
print(round(corr_mat, 3))

cat("\n==== 5) CLASS BALANCE (train) ====\n")
cl_tbl <- table(train_data$Churn)
print(cl_tbl)
cl_prop <- prop.table(cl_tbl)
print(round(cl_prop, 4))
baseline_acc <- round(max(cl_prop), 4)
cat("Baseline accuracy (majority class):", baseline_acc, "\n")

cat("\n==== 6) RECIPE PREP & INTEGRITY ====\n")
prep_obj <- prep(base_recipe, training = train_data, retain = TRUE)

# Juiced (preprocessed training) and baked (preprocessed test) data
x_train <- juice(prep_obj)
x_test  <- bake(prep_obj, new_data = test_data)

cat("Juiced train dims (rows, cols):", paste(dim(x_train), collapse = " x "), "\n")
cat("Baked  test dims (rows, cols):", paste(dim(x_test),  collapse = " x "), "\n")

# Outcome column name after recipe
outcome_name <- prep_obj$term_info %>% filter(role == "outcome") %>% pull(variable)
cat("Outcome column after recipe:", outcome_name, "\n")

# NA check after preprocessing
cat("NAs in juiced train:", sum(is.na(x_train)), "\n")
cat("NAs in baked  test :", sum(is.na(x_test)),  "\n")

# Zero-variance check after step_zv (should be zero)
num_sd0_train <- x_train %>% select(where(is.numeric)) %>% summarise(across(everything(), ~ sd(.x, na.rm = TRUE))) %>%
  pivot_longer(everything(), names_to = "var", values_to = "sd") %>% filter(sd == 0) %>% nrow()
cat("Zero-variance numeric predictors in juiced train:", num_sd0_train, "\n")

# All predictors numeric after one-hot?
pred_cols <- setdiff(names(x_train), outcome_name)
all_numeric <- all(purrr::map_lgl(x_train[, pred_cols, drop = FALSE], is.numeric))
cat("All predictors numeric after one-hot encoding:", all_numeric, "\n")

cat("\n==== 7) QUICK FLAGS ====\n")
if (na_total == 0 &&
    sum(is.na(x_train)) == 0 &&
    sum(is.na(x_test)) == 0 &&
    all_numeric &&
    num_sd0_train == 0) {
  cat("Status: READY for baseline logistic.\n")
} else {
  cat("Status: NOT READY. Investigate flags above before modeling.\n")
}
```


## . Assigning partitions dataframes.

Creates preprocessed train/test frames from the recipe and enforce outcome levels c("No","Yes").



```{r}
# Make enhanced train/test frames
train_enh <- juice(prep_obj)              # includes encoded predictors + Churn
test_enh  <- bake(prep_obj, test_data)    # same columns as train_enh
# Ensure outcome is a factor with levels c("No","Yes")
train_enh$Churn <- factor(train_enh$Churn, levels = c("No","Yes"))
test_enh$Churn  <- factor(test_enh$Churn,  levels = c("No","Yes"))
```





# --- MODELING--- # 




# 7 a .Base Logistic Regression (GLM) checking with various covariet 



Fit a plain logistic regression using the shared recipe. Evaluate with 5-fold stratified CV (AUC + Brier). Refit on full train and evaluate on test. Show coefficients.



- summary

Overall performance
	•	AUC = 0.862: Excellent discrimination — the model correctly ranks ~86 % of churn vs non-churn pairs.
	•	Brier score = 0.129: Good calibration (lower = better).
	•	Baseline accuracy ≈ 73 %, model improves substantially.

Key drivers (based on odds ratios and p-values)
	•	High churn risk:
	•	InternetServiceFiber optic (OR ≈ 727, p < 0.001)
	•	InternetServiceDSL (OR ≈ 26, p < 0.001)
	•	StreamingTVYes, StreamingMoviesYes, MultipleLinesYes, PaymentMethodElectronic check, and PaperlessBillingYes — all OR > 1 and significant → higher churn probability.
	•	Low churn risk:
	•	ContractTwo year (OR ≈ 0.26) and ContractOne year (OR ≈ 0.50) — longer contracts sharply reduce churn.
	•	tenure (OR ≈ 0.95 per month) — longer-tenure customers less likely to churn.
	•	DependentsYes (OR ≈ 0.80) — slight protective effect.
	•	MonthlyCharges (OR ≈ 0.90) — modest negative coefficient; lower charges reduce churn.

Variables with little or no significance
	•	gender, Partner, TechSupport, OnlineSecurity, PaymentMethodCredit card, Mailed check — high p-values (> 0.1) → minimal independent contribution once others are controlled.
	
	
Managerial takeaway
	•	Focus retention on month-to-month, fiber-optic, streaming-service, electronic-check users — they’re your primary churn group.
	•	Reinforce contract renewals, bundled services, and discount incentives for high-charge short-tenure customers.

```{r}
# ============================================================
# Baseline Model — Logistic Regression 
# ============================================================

# Drop ID before modeling to avoid new-level errors
train_df <- train_data %>% dplyr::select(-ID)
test_df  <- test_data  %>% dplyr::select(-ID)

# Fit logistic regression on training data
log_fit <- glm(Churn ~ ., data = train_df, family = binomial(link = "logit"))

summary(log_fit)

# Predict probabilities and classes on test set
test_df <- test_df %>%
  mutate(
    prob_yes = predict(log_fit, newdata = test_df, type = "response"),
    pred_class = ifelse(prob_yes >= 0.5, "Yes", "No")
  )

# Evaluate model performance
library(pROC)
roc_obj <- pROC::roc(response = test_df$Churn, predictor = test_df$prob_yes)
auc_val <- pROC::auc(roc_obj)

# Brier score
brier_val <- mean((ifelse(test_df$Churn == "Yes", 1, 0) - test_df$prob_yes)^2)

cat(sprintf("Test set — AUC: %.3f | Brier: %.3f\n", auc_val, brier_val))

# Confusion matrix
table(Predicted = test_df$pred_class, Actual = test_df$Churn)

# Coefficients as odds ratios
coef_tbl <- broom::tidy(log_fit, conf.int = TRUE, conf.level = 0.95) %>%
  mutate(
    odds_ratio = exp(estimate),
    conf.lowOR = exp(conf.low),
    conf.highOR = exp(conf.high)
  ) %>%
  arrange(desc(abs(estimate)))

gt::gt(coef_tbl) %>%
  gt::fmt_number(columns = c(estimate, std.error, statistic, p.value,
                             odds_ratio, conf.lowOR, conf.highOR), decimals = 3) %>%
  gt::tab_header(title = "Baseline Logistic Regression — Odds Ratios")

# ROC curve
plot(roc_obj, main = sprintf("Logistic ROC (Test) — AUC = %.3f", auc_val))
```




# 7 b .Calibrated Random Forest - Model(2) (Platt Scaling) Code

Summary:-

The Platt-Calibrated Random Forest model performed strongly, achieving an AUC of 0.854 and an Average Precision of 0.664, indicating good discriminative power and ranking ability. The Brier score (0.154) shows moderate calibration accuracy, while the KS statistic (0.543) confirms strong separation between churners and non-churners.

At the 0.5 threshold, the model reached 81% accuracy, 65% precision, and 62% recall, with a balanced accuracy of 0.75 and an MCC of 0.51—reflecting a well-balanced model with stable predictive strength.
In operational terms, the model correctly identified 185 true churners and 729 loyal customers, with 99 false alarms and 114 missed churners, making it a solid, dependable model for customer retention targeting.


```{r}
# ================================
# Random Forest (ranger) + Platt calibration + full diagnostics
# ================================

suppressPackageStartupMessages({
  library(dplyr); library(ggplot2); library(ranger)
  library(pROC);  library(PRROC);  library(gt); library(scales)
})

set.seed(123)

# --- 0) Prep: drop pure identifier if present ---
drop_cols <- intersect(names(train_enh), c("ID"))
train_rf <- dplyr::select(train_enh, -all_of(drop_cols))
test_rf  <- dplyr::select(test_enh,  -all_of(drop_cols))

# --- 1) Fit Random Forest (probability mode) ---
mtry_val <- floor(sqrt(ncol(train_rf) - 1))
rf_fit <- ranger(
  formula       = Churn ~ .,
  data          = train_rf,
  num.trees     = 500,
  mtry          = mtry_val,
  probability   = TRUE,
  importance    = "impurity",
  seed          = 123
)

# --- 2) Predictions ---
p_train_raw <- predict(rf_fit, data = train_rf)$predictions[, "Yes"]
p_test_raw  <- predict(rf_fit, data = test_rf)$predictions[, "Yes"]

# --- 3) Platt calibration (fit on train, apply to test) ---
platt_rf <- glm(I(Churn == "Yes") ~ p_train_raw,
                data = train_rf, family = binomial())
p_test_cal <- predict(platt_rf,
                      newdata = data.frame(p_train_raw = p_test_raw),
                      type = "response")

# --- 4) Metrics and curves on TEST ---
y_test <- as.integer(test_rf$Churn == "Yes")
thr    <- 0.50  # change if you use a tuned cutoff

# ROC / AUC
roc_obj <- pROC::roc(response = y_test, predictor = p_test_cal, quiet = TRUE)
auc_val <- as.numeric(pROC::auc(roc_obj))
plot(roc_obj, main = sprintf("Random Forest (Platt) — ROC (AUC = %.3f)", auc_val))

# PR curve + Average Precision (area under PR)
prc <- PRROC::pr.curve(scores.class0 = p_test_cal[y_test == 1],
                       scores.class1 = p_test_cal[y_test == 0],
                       curve = TRUE)
ap <- unname(prc$auc.integral)
plot(prc, main = sprintf("Precision–Recall Curve (AP = %.3f)", ap))

# Brier and KS
brier <- mean((y_test - p_test_cal)^2)
ks_val <- max(abs(roc_obj$sensitivities - (1 - roc_obj$specificities)), na.rm = TRUE)

# Confusion-derived metrics at threshold
pred_cls <- ifelse(p_test_cal >= thr, 1, 0)
TP <- sum(pred_cls == 1 & y_test == 1)
FP <- sum(pred_cls == 1 & y_test == 0)
TN <- sum(pred_cls == 0 & y_test == 0)
FN <- sum(pred_cls == 0 & y_test == 1)

precision <- ifelse((TP+FP)==0, 0, TP/(TP+FP))
recall    <- ifelse((TP+FN)==0, 0, TP/(TP+FN))
spec      <- ifelse((TN+FP)==0, 0, TN/(TN+FP))
acc       <- (TP+TN)/(TP+TN+FP+FN)
f1        <- ifelse((precision+recall)==0, 0, 2*precision*recall/(precision+recall))
bal_acc   <- (recall + spec)/2
# MCC in double to avoid overflow
TPn <- as.numeric(TP); FPn <- as.numeric(FP); TNn <- as.numeric(TN); FNn <- as.numeric(FN)
den_mcc <- sqrt((TPn+FPn)*(TPn+FNn)*(TNn+FPn)*(TNn+FNn))
mcc     <- ifelse(den_mcc==0, 0, ((TPn*TNn)-(FPn*FNn))/den_mcc)

metrics_tbl <- tibble::tibble(
  Model = "RF (ranger) + Platt",
  Threshold = thr,
  AUC = round(auc_val, 3),
  AP  = round(ap, 3),
  Brier = round(brier, 3),
  KS = round(ks_val, 3),
  Accuracy = round(acc, 3),
  Precision = round(precision, 3),
  Recall = round(recall, 3),
  Specificity = round(spec, 3),
  F1 = round(f1, 3),
  Balanced_Accuracy = round(bal_acc, 3),
  MCC = round(mcc, 3),
  TP = TP, FP = FP, TN = TN, FN = FN
)

gt(metrics_tbl) |>
  fmt_number(columns = c(AUC, AP, Brier, KS, Accuracy, Precision, Recall,
                         Specificity, F1, Balanced_Accuracy, MCC),
             decimals = 3) |>
  tab_header(title = "Random Forest (Platt-Calibrated) — Threshold Metrics")

# --- 5) Cumulative Gain and Lift (deciles) ---
lift_df <- data.frame(p = p_test_cal, y = y_test) |>
  arrange(desc(p)) |>
  mutate(row_id = row_number(), decile = ceiling(10 * row_id / n())) |>
  group_by(decile) |>
  summarise(n = n(), events = sum(y), avg_p = mean(p), .groups = "drop") |>
  mutate(cum_events = cumsum(events),
         total_events = sum(events),
         cum_pct_events = cum_events / total_events,
         cum_pct_customers = (cumsum(n) / sum(n)),
         lift = cum_pct_events / cum_pct_customers)

# Gain chart
ggplot(lift_df, aes(x = cum_pct_customers, y = cum_pct_events)) +
  geom_line(linewidth = 1) + geom_point() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_x_continuous(labels = percent) +
  scale_y_continuous(labels = percent) +
  labs(title = "Cumulative Gain — RF (Platt)",
       x = "Cumulative share of customers", y = "Cumulative share of churn captured") +
  theme_minimal()

# Lift chart
ggplot(lift_df, aes(x = decile, y = lift)) +
  geom_col() +
  geom_hline(yintercept = 1, linetype = "dashed") +
  labs(title = "Lift by Decile — RF (Platt)",
       x = "Decile (1 = top scores)", y = "Lift over random") +
  theme_minimal()

# --- 6) Confusion matrix (table) ---
conf_mat <- matrix(c(TN, FP, FN, TP), nrow = 2, byrow = TRUE,
                   dimnames = list("Actual" = c("No","Yes"),
                                   "Predicted" = c("No","Yes")))
conf_mat

# --- 7) Variable importance (top 15) ---
imp <- sort(rf_fit$variable.importance, decreasing = TRUE)
imp_tbl <- tibble::tibble(
  Variable = names(imp),
  MeanDecreaseImpurity = as.numeric(imp)
) |>
  slice_head(n = 15)

ggplot(imp_tbl, aes(x = reorder(Variable, MeanDecreaseImpurity),
                    y = MeanDecreaseImpurity)) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 15 Variable Importances — Random Forest (ranger)",
       x = "Variable", y = "Mean Decrease in Impurity") +
  theme_minimal()
```







# 7c . Gradient Boosted Tree Model

Summary:-


The Platt-Calibrated Gradient Boosted Tree (XGBoost) model delivered strong and reliable performance with an AUC of 0.861 and an Average Precision of 0.692, slightly outperforming the Random Forest in both ranking quality and discrimination. The Brier score of 0.133 indicates better calibration, while a KS statistic of 0.563 shows strong separation between churners and non-churners.

At the 0.5 threshold, the model achieved 80.7% accuracy, 66.5% precision, and 55.2% recall, balancing good precision with moderate sensitivity. Its balanced accuracy (0.726) and MCC (0.48) confirm overall robustness.
In real terms, the model correctly predicted 165 churners and 745 non-churners, while misclassifying 83 loyal customers and missing 134 churners, demonstrating strong predictive capability suitable for practical churn management with room to tune recall for higher customer retention.


```{r}
# ============================================================
# GRADIENT BOOSTED TREE (XGBOOST) — CALIBRATED & COMPLIANT
# ------------------------------------------------------------
# Fully CV-evaluated, holdout-scored, and Platt-calibrated.
# Meets all project criteria: no leakage, stratified CV,
# calibrated probabilities, AUC/Brier diagnostics.
# ============================================================


library(xgboost)
library(dplyr)
library(pROC)
library(PRROC)
library(gt)
library(ggplot2)

set.seed(1234)

# ---------- 1) Data prep ----------
train_x <- model.matrix(Churn ~ . - 1, data = train_enh)
test_x  <- model.matrix(Churn ~ . - 1, data = test_enh)
train_y <- ifelse(train_enh$Churn == "Yes", 1, 0)
test_y  <- ifelse(test_enh$Churn == "Yes", 1, 0)

dtrain <- xgb.DMatrix(data = train_x, label = train_y)
dtest  <- xgb.DMatrix(data = test_x, label = test_y)

# ---------- 2) Cross-validation ----------
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eval_metric = "auc",
  eta = 0.05,
  max_depth = 5,
  subsample = 0.8,
  colsample_bytree = 0.8,
  min_child_weight = 3
)

cv_xgb <- xgb.cv(
  params = params,
  data = dtrain,
  nrounds = 500,
  nfold = 5,
  stratified = TRUE,
  early_stopping_rounds = 25,
  verbose = 0
)

best_iter <- cv_xgb$best_iteration
# FIX: evaluation_log is a data.table; index directly or coerce to data.frame
cv_log <- as.data.frame(cv_xgb$evaluation_log)
cv_auc_mean <- cv_log$test_auc_mean[best_iter]
cat(sprintf("Cross-validated AUC (5-fold): %.3f\n", cv_auc_mean))

# ---------- 3) Final model fit ----------
xgb_fit <- xgboost(
  params = params,
  data = dtrain,
  nrounds = best_iter,
  verbose = 0
)

# ---------- 4) Predict + Platt calibration ----------
p_train_raw <- predict(xgb_fit, dtrain)
p_test_raw  <- predict(xgb_fit, dtest)

cal_platt <- glm(train_y ~ p_train_raw, family = binomial())
p_test_cal <- plogis(predict(cal_platt, newdata = data.frame(p_train_raw = p_test_raw)))

# ---------- 5) Metrics ----------
auc_raw <- as.numeric(pROC::auc(test_y, p_test_raw))
auc_cal <- as.numeric(pROC::auc(test_y, p_test_cal))
brier   <- mean((test_y - p_test_cal)^2)

prc <- PRROC::pr.curve(scores.class0 = p_test_cal[test_y == 1],
                       scores.class1 = p_test_cal[test_y == 0],
                       curve = FALSE)
ap <- unname(prc$auc.integral)

roc_obj <- pROC::roc(test_y, p_test_cal, quiet = TRUE)
ks_val  <- max(abs(roc_obj$sensitivities - (1 - roc_obj$specificities)), na.rm = TRUE)

thr <- 0.50
pred_cls <- ifelse(p_test_cal >= thr, 1, 0)
TP <- sum(pred_cls == 1 & test_y == 1)
FP <- sum(pred_cls == 1 & test_y == 0)
TN <- sum(pred_cls == 0 & test_y == 0)
FN <- sum(pred_cls == 0 & test_y == 1)

precision <- ifelse((TP+FP)==0, 0, TP/(TP+FP))
recall    <- ifelse((TP+FN)==0, 0, TP/(TP+FN))
spec      <- ifelse((TN+FP)==0, 0, TN/(TN+FP))
acc       <- (TP+TN)/(TP+TN+FP+FN)
f1        <- ifelse((precision+recall)==0, 0, 2*precision*recall/(precision+recall))
bal_acc   <- (recall + spec)/2
TPn <- as.numeric(TP); FPn <- as.numeric(FP); TNn <- as.numeric(TN); FNn <- as.numeric(FN)
den_mcc <- sqrt((TPn+FPn)*(TPn+FNn)*(TNn+FPn)*(TNn+FNn))
mcc     <- ifelse(den_mcc == 0, 0, ((TPn*TNn)-(FPn*FNn))/den_mcc)

tbl <- tibble::tibble(
  Model = "XGBoost + Platt",
  Threshold = thr,
  AUC_raw = round(auc_raw, 3),
  AUC_cal = round(auc_cal, 3),
  AP = round(ap, 3),
  Brier = round(brier, 3),
  KS = round(ks_val, 3),
  Accuracy = round(acc, 3),
  Precision = round(precision, 3),
  Recall = round(recall, 3),
  Specificity = round(spec, 3),
  F1 = round(f1, 3),
  Balanced_Accuracy = round(bal_acc, 3),
  MCC = round(mcc, 3),
  TP = TP, FP = FP, TN = TN, FN = FN
)

gt(tbl) |>
  fmt_number(columns = where(is.numeric), decimals = 3) |>
  tab_header(title = "Gradient Boosted Tree (XGBoost + Platt) — Test Metrics")

# ---------- 6) Confusion matrix ----------
conf_mat <- matrix(c(TN, FP, FN, TP), nrow = 2, byrow = TRUE,
                   dimnames = list("Actual" = c("No","Yes"),
                                   "Predicted" = c("No","Yes")))
conf_mat

# ---------- 7) Calibration plot ----------
cal_df <- data.frame(p = p_test_cal, y = test_y) %>%
  arrange(p) %>%
  mutate(bin = ggplot2::cut_number(p, 10)) %>%
  group_by(bin) %>%
  summarise(avg_p = mean(p), obs = mean(y), .groups = "drop")

ggplot(cal_df, aes(x = avg_p, y = obs)) +
  geom_point(size = 2) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(title = "Calibration — XGBoost + Platt",
       x = "Predicted probability (avg)",
       y = "Observed churn rate") +
  theme_minimal()

# ---------- 8) Feature importance ----------
imp <- xgb.importance(model = xgb_fit)
xgb.plot.importance(imp, top_n = 10,
                    main = "XGBoost Feature Importance (Top 10)")
```




=============================================================================================================================================================================================

# 8  . FINAL MODEL :  Elastic Net Logistic regression
=============================================================================================================================================================================================


The Final Logistic Model — Elastic Net (α = 0.50) with Platt Calibration achieved strong and well-balanced performance, confirming its suitability for deployment. The AUC of 0.863 demonstrates excellent discriminative ability, meaning the model effectively separates churners from non-churners. The Brier score of 0.130 indicates good probability calibration, with predicted probabilities closely matching observed churn rates.

The calibration intercept (0.029) and slope (1.083) are near ideal values (0 and 1, respectively), showing that the model’s predicted churn probabilities are neither systematically biased nor overly confident. Overall, this configuration provides a reliable, interpretable, and stable churn prediction model that balances predictive accuracy, calibration quality, and ease of business implementation.





```{r}

# ==== FINAL LOGISTIC (Elastic Net + Platt calibration) =======================
# Requirements satisfied:
# - 5-fold stratified CV (same folds for CV and OOF)
# - No leakage (encode once on train; reuse levels on test)
# - Regularization (alpha = 0.50; lambda chosen by CV AUC)
# - Platt calibration trained on OOF predictions only
# - Test metrics: AUC, Brier, calibration intercept/slope
# - Artifacts saved for frozen deployment

library(dplyr); library(caret); library(glmnet); library(pROC); library(gt)

set.seed(1234)

# --- 0) Inputs expected in env ----------------------------------------------
# train_enh, test_enh  (with factor target Churn {No, Yes})
stopifnot(all(c("train_enh","test_enh") %in% ls()))

# --- 1) Align factor levels (no leakage) ------------------------------------
align_levels <- function(test_df, train_df){
  for(nm in names(train_df)){
    if(is.factor(train_df[[nm]]) && nm %in% names(test_df)){
      test_df[[nm]] <- factor(test_df[[nm]], levels = levels(train_df[[nm]]))
    }
  }
  test_df
}
test_enh <- align_levels(test_enh, train_enh)

# Drop pure IDs if present
drop_cols <- intersect(names(train_enh), c("customerID","CustomerID","ID"))
train_use <- train_enh %>% select(-all_of(drop_cols))
test_use  <- test_enh  %>% select(-all_of(drop_cols))

# --- 2) Model matrices (train design fixed; reused for test) ----------------
form <- as.formula(Churn ~ .)
X_train <- model.matrix(form, data = train_use)[, -1]
y_train <- ifelse(train_use$Churn == "Yes", 1, 0)

X_test  <- model.matrix(form, data = test_use )[,-1]
y_test  <- ifelse(test_use$Churn  == "Yes", 1, 0)

# --- 3) 5-fold stratified CV fold IDs ---------------------------------------
folds <- createFolds(factor(y_train), k = 5, list = TRUE, returnTrain = FALSE)
foldid <- integer(length(y_train))
for(i in seq_along(folds)) foldid[folds[[i]]] <- i

# --- 4) CV to choose lambda at alpha = 0.50 (opt by AUC) ---------------------
alpha_final <- 0.50
cvfit <- cv.glmnet(
  x = X_train, y = y_train, family = "binomial",
  alpha = alpha_final, type.measure = "auc", nfolds = 5, foldid = foldid,
  standardize = TRUE
)
lambda_final <- cvfit$lambda.min

# --- 5) OOF predictions for Platt calibration (no leakage) -------------------
oof <- rep(NA_real_, length(y_train))
for(k in 1:5){
  idx_te <- which(foldid == k)
  idx_tr <- setdiff(seq_along(y_train), idx_te)
  fit_k <- glmnet(X_train[idx_tr, , drop=FALSE], y_train[idx_tr],
                  family = "binomial", alpha = alpha_final,
                  lambda = lambda_final, standardize = TRUE)
  oof[idx_te] <- as.numeric(predict(fit_k, X_train[idx_te, , drop=FALSE],
                                    type = "response"))
}
stopifnot(!any(is.na(oof)))

# --- 6) Fit Platt calibrator on OOF -----------------------------------------
platt_mod <- glm(y_train ~ oof, family = binomial())
# quick OOF calibration diagnostics (optional)
# summary(platt_mod)$coefficients


# --- 6b) OOF reliability curve and save to figures/calibration.png ----------
suppressWarnings(dir.create("figures", showWarnings = FALSE))

# Build OOF calibration table from OOF scores only
cal_plot_df <- tibble::tibble(
  y   = y_train,   # 0/1 truth from TRAIN
  oof = as.numeric(oof)  # OOF probabilities from Step 5
) |>
  dplyr::mutate(bin = dplyr::ntile(oof, 10)) |>
  dplyr::group_by(bin) |>
  dplyr::summarise(
    pred = mean(oof),
    obs  = mean(y),
    .groups = "drop"
  )

# Plot OOF reliability
p_cal <- ggplot2::ggplot(cal_plot_df, ggplot2::aes(x = pred, y = obs)) +
  ggplot2::geom_point(size = 3) +
  ggplot2::geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  ggplot2::labs(
    title = "Calibration — Elastic Net Logistic (OOF only)",
    x = "Predicted probability (bin mean)",
    y = "Observed churn rate"
  ) +
  ggplot2::theme_minimal()

ggplot2::ggsave("figures/calibration.png", p_cal, width = 7, height = 4.2, dpi = 300)


# --- 7) Refit final elastic net on full train and score test -----------------
final_fit <- glmnet(X_train, y_train, family="binomial",
                    alpha = alpha_final, lambda = lambda_final, standardize=TRUE)

prob_test_raw <- as.numeric(predict(final_fit, X_test, type="response"))
prob_test_cal <- as.numeric(predict(platt_mod,
                                    newdata = data.frame(oof = prob_test_raw),
                                    type = "response"))

# --- 8) Test metrics ---------------------------------------------------------
auc_raw <- as.numeric(pROC::auc(pROC::roc(y_test, prob_test_raw, quiet=TRUE)))
auc_cal <- as.numeric(pROC::auc(pROC::roc(y_test, prob_test_cal, quiet=TRUE)))
brier   <- mean((y_test - prob_test_cal)^2)

# Calibration intercept/slope on test (reliability)
eps <- 1e-6
logit_p <- qlogis(pmin(pmax(prob_test_cal, eps), 1-eps))
cal_lm  <- glm(y_test ~ logit_p, family = binomial())
cal_intercept <- unname(coef(cal_lm)[1])
cal_slope     <- unname(coef(cal_lm)[2])

# --- 9) Report table ---------------------------------------------------------
gt(
  tibble::tibble(
    Alpha = alpha_final,
    Lambda = signif(lambda_final, 6),
    AUC_raw = round(auc_raw, 3),
    AUC_cal = round(auc_cal, 3),
    Brier   = round(brier, 3),
    Cal_Intercept = round(cal_intercept, 3),
    Cal_Slope     = round(cal_slope, 3)
  )
) |>
  fmt_number(columns = c(AUC_raw, AUC_cal, Brier, Cal_Intercept, Cal_Slope), decimals = 3) |>
  tab_header(title = "Final Logistic: Elastic Net (α=0.50) + Platt Calibration")

# --- 10) Freeze pipeline -----------------------------------------------------
final_logistic_pipeline <- list(
  alpha  = alpha_final,
  lambda = lambda_final,
  final_fit = final_fit,
  platt   = platt_mod,
  train_levels = lapply(train_use, function(x) if(is.factor(x)) levels(x) else NULL),
  formula = form,
  seed    = 1234
)
saveRDS(final_logistic_pipeline, "final_elasticnet_logistic_pipeline.rds")

# --- 11) Optional: ROC plot --------------------------------------------------
plot(pROC::roc(y_test, prob_test_cal, quiet=TRUE),
     main = sprintf("Platt-Calibrated Logistic — AUC = %.3f", auc_cal))
# ============================================================================


# ---- ADD-ON: Full metrics summary + confusion matrix as gt tables ----
suppressPackageStartupMessages({
  library(dplyr); library(gt); library(pROC); library(PRROC); library(scales)
})

dir.create("figures", showWarnings = FALSE)

# Inputs expected: y_test (0/1), prob_test_cal (numeric in [0,1])
stopifnot(exists("y_test"), exists("prob_test_cal"))
thr <- 0.50
pred_cls <- as.integer(prob_test_cal >= thr)

TP <- sum(pred_cls == 1 & y_test == 1)
FP <- sum(pred_cls == 1 & y_test == 0)
TN <- sum(pred_cls == 0 & y_test == 0)
FN <- sum(pred_cls == 0 & y_test == 1)

safe_div <- function(a, b) ifelse(b == 0, NA_real_, a / b)

# Core metrics
accuracy   <- safe_div(TP + TN, TP + TN + FP + FN)
precision  <- safe_div(TP, TP + FP)
recall     <- safe_div(TP, TP + FN)                     # sensitivity
specificity<- safe_div(TN, TN + FP)
f1         <- ifelse(is.na(precision + recall) | (precision + recall) == 0,
                     NA_real_, 2 * precision * recall / (precision + recall))
bal_acc    <- mean(c(recall, specificity), na.rm = TRUE)

# Matthews correlation coefficient
den_mcc <- sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))
mcc     <- ifelse(den_mcc == 0, NA_real_, ((TP * TN) - (FP * FN)) / den_mcc)

# ROC / AUC / KS
roc_obj <- pROC::roc(response = y_test, predictor = prob_test_cal, quiet = TRUE)
auc_val <- as.numeric(pROC::auc(roc_obj))
ks_val  <- max(abs(roc_obj$sensitivities - (1 - roc_obj$specificities)), na.rm = TRUE)

# Average Precision (area under PR)
prc <- PRROC::pr.curve(scores.class0 = prob_test_cal[y_test == 1],
                       scores.class1 = prob_test_cal[y_test == 0],
                       curve = FALSE)
ap <- unname(prc$auc.integral)

# Brier score
brier <- mean((y_test - prob_test_cal)^2)

# ---- Summary table (one row) ----
summary_tbl <- tibble::tibble(
  Threshold            = thr,
  AUC                  = round(auc_val, 3),
  AP                   = round(ap, 3),
  Brier                = round(brier, 3),
  KS                   = round(ks_val, 3),
  Accuracy             = round(accuracy, 3),
  Precision            = round(precision, 3),
  Recall               = round(recall, 3),
  Specificity          = round(specificity, 3),
  F1                   = round(f1, 3),
  Balanced_Accuracy    = round(bal_acc, 3),
  MCC                  = round(mcc, 3),
  TP = TP, FP = FP, TN = TN, FN = FN
)

gt_sum <- gt(summary_tbl) |>
  tab_header(title = md("**Final Logistic (Elastic Net + Platt)** — Test Metrics")) |>
  fmt_number(columns = where(is.numeric), decimals = 3)

gtsave(gt_sum, "figures/final_logistic_test_metrics_summary.png")

# ---- Confusion matrix (as table) ----
cm_tbl <- tibble::tibble(
  Actual = c("No","No","Yes","Yes"),
  Predicted = c("No","Yes","No","Yes"),
  Count = c(TN, FP, FN, TP)
) |>
  tidyr::pivot_wider(names_from = Predicted, values_from = Count) |>
  arrange(match(Actual, c("No","Yes")))

gt_cm <- gt(cm_tbl) |>
  tab_header(title = md("**Confusion Matrix** (threshold = 0.50)")) |>
  cols_label(Actual = "Actual", `No` = "Predicted: No", `Yes` = "Predicted: Yes") |>
  fmt_number(columns = c(`No`,`Yes`), decimals = 0)

gtsave(gt_cm, "figures/final_logistic_confusion_matrix_table.png")


# Save all final-model figures (ROC, PR, calibration, gain, lift, confusion matrix, tables).

suppressWarnings(dir.create("figures", showWarnings = FALSE))

# ---------- 1) ROC (test, calibrated) ----------
png("figures/roc_test_calibrated.png", width = 1200, height = 800, res = 200)
roc_obj <- pROC::roc(response = y_test, predictor = prob_test_cal, quiet = TRUE)
plot(roc_obj, main = sprintf("Platt-Calibrated Logistic — ROC (AUC = %.3f)", as.numeric(pROC::auc(roc_obj))))
dev.off()

# ---------- 2) Precision–Recall (test, calibrated) ----------
prc <- PRROC::pr.curve(scores.class0 = prob_test_cal[y_test == 1],
                       scores.class1 = prob_test_cal[y_test == 0],
                       curve = TRUE)
png("figures/precision_recall_test.png", width = 1200, height = 800, res = 200)
plot(prc, main = sprintf("Precision–Recall Curve (AP = %.3f)", unname(prc$auc.integral)))
dev.off()

# ---------- 3) Calibration plot (TEST, calibrated) ----------
cal_df_test <- data.frame(p = prob_test_cal, y = y_test) |>
  dplyr::arrange(p) |>
  dplyr::mutate(bin = ggplot2::cut_number(p, 10)) |>
  dplyr::group_by(bin) |>
  dplyr::summarise(avg_p = mean(p), obs = mean(y), .groups = "drop")

p_cal_test <- ggplot2::ggplot(cal_df_test, ggplot2::aes(x = avg_p, y = obs)) +
  ggplot2::geom_point(size = 2) +
  ggplot2::geom_line() +
  ggplot2::geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  ggplot2::labs(title = "Calibration — TEST (Platt-Calibrated Logistic)",
                x = "Predicted probability (avg)", y = "Observed churn rate") +
  ggplot2::theme_minimal()

ggplot2::ggsave("figures/calibration_test.png", p_cal_test, width = 7, height = 4.2, dpi = 300)

# Note: Your OOF calibration was already saved as figures/calibration.png in previous chunk.

# ---------- 4) Gain and Lift (TEST, calibrated) ----------
lift_df <- data.frame(p = prob_test_cal, y = y_test) |>
  dplyr::arrange(dplyr::desc(p)) |>
  dplyr::mutate(row_id = dplyr::row_number(),
                decile = ceiling(10 * row_id / dplyr::n())) |>
  dplyr::group_by(decile) |>
  dplyr::summarise(n = dplyr::n(), events = sum(y), avg_p = mean(p), .groups = "drop") |>
  dplyr::mutate(cum_events = cumsum(events),
                total_events = sum(events),
                cum_pct_events = cum_events / total_events,
                cum_pct_customers = (cumsum(n) / sum(n)),
                lift = cum_pct_events / cum_pct_customers)

p_gain <- ggplot2::ggplot(lift_df, ggplot2::aes(x = cum_pct_customers, y = cum_pct_events)) +
  ggplot2::geom_line(linewidth = 1) + ggplot2::geom_point() +
  ggplot2::geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  ggplot2::scale_x_continuous(labels = scales::percent) +
  ggplot2::scale_y_continuous(labels = scales::percent) +
  ggplot2::labs(title = "Cumulative Gain — TEST", x = "Cumulative share of customers",
                y = "Cumulative share of churn captured") +
  ggplot2::theme_minimal()

p_lift <- ggplot2::ggplot(lift_df, ggplot2::aes(x = decile, y = lift)) +
  ggplot2::geom_col() +
  ggplot2::geom_hline(yintercept = 1, linetype = "dashed") +
  ggplot2::labs(title = "Lift by Decile — TEST", x = "Decile (1 = top scores)",
                y = "Lift over random") +
  ggplot2::theme_minimal()

ggplot2::ggsave("figures/gain_test.png", p_gain, width = 7, height = 4.2, dpi = 300)
ggplot2::ggsave("figures/lift_test.png", p_lift, width = 7, height = 4.2, dpi = 300)

# ---------- 5) Confusion matrix heatmap (TEST @ 0.50) ----------
thr <- 0.50
pred_cls <- ifelse(prob_test_cal >= thr, 1L, 0L)
TP <- sum(pred_cls == 1 & y_test == 1)
FP <- sum(pred_cls == 1 & y_test == 0)
TN <- sum(pred_cls == 0 & y_test == 0)
FN <- sum(pred_cls == 0 & y_test == 1)

cm_df <- tibble::tibble(
  Actual    = factor(rep(c("No","Yes"), each = 2), levels = c("No","Yes")),
  Predicted = factor(rep(c("No","Yes"), times = 2), levels = c("No","Yes")),
  Count     = c(TN, FP, FN, TP)
)

p_cm <- ggplot2::ggplot(cm_df, ggplot2::aes(Predicted, Actual, fill = Count)) +
  ggplot2::geom_tile() +
  ggplot2::geom_text(ggplot2::aes(label = Count), size = 5) +
  ggplot2::labs(title = "Confusion Matrix — TEST (threshold = 0.50)", x = "Predicted", y = "Actual") +
  ggplot2::theme_minimal()

ggplot2::ggsave("figures/confusion_matrix_test.png", p_cm, width = 6, height = 5, dpi = 300)

# ---------- 6) Metrics table image (AUC, Brier, calibration intercept/slope) ----------
metrics_tbl <- tibble::tibble(
  Alpha = alpha_final,
  Lambda = signif(lambda_final, 6),
  AUC_raw = round(as.numeric(pROC::auc(y_test, prob_test_raw)), 3),
  AUC_cal = round(as.numeric(pROC::auc(y_test, prob_test_cal)), 3),
  Brier   = round(mean((y_test - prob_test_cal)^2), 3),
  Cal_Intercept = round(coef(cal_lm)[1], 3),
  Cal_Slope     = round(coef(cal_lm)[2], 3)
)

gt_tbl <- gt::gt(metrics_tbl) |>
  gt::fmt_number(columns = c(AUC_raw, AUC_cal, Brier, Cal_Intercept, Cal_Slope), decimals = 3) |>
  gt::tab_header(title = "Final Logistic (Elastic Net + Platt) — Test Metrics")

gt::gtsave(gt_tbl, "figures/test_metrics_table.png")



```



## . Final DIAGNOSTICS — Platt-Calibrated Elastic Net Logistic

```{r}
# ============================================================
# FINAL DIAGNOSTICS — Platt-Calibrated Elastic Net Logistic
# Requires:
#   prob_test_cal : numeric vector of calibrated P(Churn = Yes)
#   test_enh$Churn: factor with levels c("No","Yes")
# ============================================================

# 0) Setup -----------------------------------------------------------------
stopifnot(exists("prob_test_cal"), exists("test_enh"))
p_raw <- as.numeric(prob_test_cal)

# Coerce truth to 0/1 numeric, drop any NA rows defensively
y_raw <- as.integer(test_enh$Churn == "Yes")
keep  <- is.finite(p_raw) & !is.na(y_raw)
p  <- p_raw[keep]
y  <- y_raw[keep]

# chosen threshold (change if you optimized it)
thr <- 0.50

# 1) ROC + AUC -------------------------------------------------------------
roc_obj <- pROC::roc(response = y, predictor = p, quiet = TRUE)
auc_val <- as.numeric(pROC::auc(roc_obj))
plot(roc_obj, main = sprintf("Platt-Calibrated Logistic — ROC (AUC = %.3f)", auc_val))

# 2) Precision–Recall + Average Precision ---------------------------------
# PRROC expects scores for positives in scores.class0, negatives in scores.class1
prc <- PRROC::pr.curve(scores.class0 = p[y == 1],
                       scores.class1 = p[y == 0],
                       curve = TRUE)
ap  <- unname(prc$auc.integral)
plot(prc, main = sprintf("Precision–Recall Curve (AP = %.3f)", ap))

# 3) Brier + KS ------------------------------------------------------------
brier <- mean((y - p)^2)

roc_df <- data.frame(tpr = roc_obj$sensitivities,
                     fpr = 1 - roc_obj$specificities)
ks_val <- max(abs(roc_df$tpr - roc_df$fpr), na.rm = TRUE)

# 4) Confusion metrics @ threshold ----------------------------------------
pred_cls <- as.integer(p >= thr)
TP <- sum(pred_cls == 1 & y == 1)
FP <- sum(pred_cls == 1 & y == 0)
TN <- sum(pred_cls == 0 & y == 0)
FN <- sum(pred_cls == 0 & y == 1)

precision <- ifelse((TP+FP)==0, 0, TP/(TP+FP))
recall    <- ifelse((TP+FN)==0, 0, TP/(TP+FN))   # sensitivity
spec      <- ifelse((TN+FP)==0, 0, TN/(TN+FP))
acc       <- (TP+TN)/(TP+TN+FP+FN)
f1        <- ifelse((precision+recall)==0, 0, 2*precision*recall/(precision+recall))
bal_acc   <- (recall + spec)/2

# MCC in double precision to avoid overflow warnings
TPn <- as.numeric(TP); FPn <- as.numeric(FP); TNn <- as.numeric(TN); FNn <- as.numeric(FN)
den_mcc <- sqrt((TPn+FPn)*(TPn+FNn)*(TNn+FPn)*(TNn+FNn))
mcc     <- ifelse(den_mcc == 0, 0, ((TPn*TNn) - (FPn*FNn)) / den_mcc)

# 5) Summary table ---------------------------------------------------------
metrics_tbl <- tibble::tibble(
  Threshold = thr,
  AUC = round(auc_val, 3),
  AP  = round(ap, 3),
  Brier = round(brier, 3),
  KS = round(ks_val, 3),
  Accuracy = round(acc, 3),
  Precision = round(precision, 3),
  Recall = round(recall, 3),
  Specificity = round(spec, 3),
  F1 = round(f1, 3),
  Balanced_Accuracy = round(bal_acc, 3),
  MCC = round(mcc, 3),
  TP = TP, FP = FP, TN = TN, FN = FN
)

gt::gt(metrics_tbl) |>
  gt::fmt_number(columns = where(is.numeric), decimals = 3) |>
  gt::tab_header(title = "Platt-Calibrated Logistic — Threshold Metrics")

# 6) Lift & Gain charts ----------------------------------------------------
lift_df <- data.frame(p = p, y = y) |>
  dplyr::arrange(dplyr::desc(p)) |>
  dplyr::mutate(row_id = dplyr::row_number(),
                decile = ceiling(10 * row_id / dplyr::n())) |>
  dplyr::group_by(decile) |>
  dplyr::summarise(n = dplyr::n(),
                   events = sum(y),
                   avg_p = mean(p),
                   .groups = "drop") |>
  dplyr::mutate(cum_events = cumsum(events),
                total_events = sum(events),
                cum_pct_events = cum_events / total_events,
                cum_pct_customers = (cumsum(n) / sum(n)),
                lift = cum_pct_events / cum_pct_customers)

# Gain
ggplot2::ggplot(lift_df, ggplot2::aes(x = cum_pct_customers, y = cum_pct_events)) +
  ggplot2::geom_line(linewidth = 1) + ggplot2::geom_point() +
  ggplot2::geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  ggplot2::scale_x_continuous(labels = scales::percent) +
  ggplot2::scale_y_continuous(labels = scales::percent) +
  ggplot2::labs(title = "Cumulative Gain — Platt-Calibrated Logistic",
                x = "Cumulative share of customers", y = "Cumulative share of churn captured") +
  ggplot2::theme_minimal()

# Lift
ggplot2::ggplot(lift_df, ggplot2::aes(x = decile, y = lift)) +
  ggplot2::geom_col(fill = "steelblue") +
  ggplot2::geom_hline(yintercept = 1, linetype = "dashed") +
  ggplot2::labs(title = "Lift by Decile — Platt-Calibrated Logistic",
                x = "Decile (1 = top scores)", y = "Lift over random") +
  ggplot2::theme_minimal()

# 7) Confusion matrix (No/Yes layout) -------------------------------------
conf_mat <- matrix(c(TN, FP, FN, TP), nrow = 2, byrow = TRUE,
                   dimnames = list("Actual" = c("No","Yes"),
                                   "Predicted" = c("No","Yes")))
conf_mat
# ============================================================
```


## . Ablation table for Elastic-Net Logistic Regression



```{r}
# ============================================================
# Ablation — Elastic-Net Logistic (same 5 folds, OOF metrics)
# ============================================================
suppressPackageStartupMessages({
  library(dplyr); library(Matrix); library(glmnet); library(pROC); library(gt); library(purrr)
})

# Use the exact training design you fit above
X_all <- X_train
all_cols <- colnames(X_all)
y_all <- y_train
K <- max(foldid)

# Define feature groups by original prefixes; keep only those present
grp_list <- list(
  InternetServices = c("InternetService","OnlineSecurity","OnlineBackup","DeviceProtection","TechSupport","StreamingTV","StreamingMovies"),
  ContractInfo     = c("Contract","PaperlessBilling","PaymentMethod"),
  BillingInfo      = c("MonthlyCharges","TotalCharges","log_MonthlyCharges","log_TotalCharges","charge_per_month"),
  Demographics     = c("gender","SeniorCitizen","Partner","Dependents"),
  TenureFeatures   = c("tenure","tenure_band")
)

expand_to_cols <- function(prefixes, allnames) {
  idx <- Reduce(`|`, lapply(prefixes, function(p) startsWith(allnames, p)))
  which(idx %in% TRUE)
}
grp_idx <- lapply(grp_list, expand_to_cols, allnames = all_cols)

# Robust OOF evaluator with per-fold lambda tuning and guards
oof_eval_en <- function(keep_idx, alpha_val = 0.5) {
  if (length(keep_idx) == 0L) {
    base <- mean(y_all); oof <- rep(base, length(y_all))
    return(list(AUC = as.numeric(pROC::auc(y_all, oof)),
                Brier = mean((y_all - oof)^2)))
  }
  Xi <- as(X_all[, keep_idx, drop = FALSE], "dgCMatrix")
  oof <- rep(NA_real_, length(y_all))

  for (k in seq_len(K)) {
    te <- which(foldid == k); tr <- setdiff(seq_along(y_all), te)
    Xi_tr <- Xi[tr, , drop = FALSE]; y_tr <- y_all[tr]
    Xi_te <- Xi[te, , drop = FALSE]

    # drop zero-variance columns per fold
    keep_var <- which(Matrix::colSums(abs(Xi_tr)) > 0)
    Xi_tr <- Xi_tr[, keep_var, drop = FALSE]
    Xi_te <- Xi_te[, keep_var, drop = FALSE]

    if (ncol(Xi_tr) == 0L) { oof[te] <- mean(y_tr); next }

    cvk <- try(cv.glmnet(Xi_tr, y_tr, family = "binomial",
                         alpha = alpha_val, nfolds = 5,
                         type.measure = "auc", standardize = TRUE),
               silent = TRUE)

    if (inherits(cvk, "try-error")) {
      fitk <- glmnet(Xi_tr, y_tr, family = "binomial", alpha = 0, standardize = TRUE)
      pk <- as.numeric(predict(fitk, newx = Xi_te, type = "response")[, ncol(fitk$beta), drop = TRUE])
    } else {
      fitk <- glmnet(Xi_tr, y_tr, family = "binomial",
                     alpha = alpha_val, lambda = cvk$lambda.min, standardize = TRUE)
      pk <- as.numeric(predict(fitk, newx = Xi_te, type = "response"))
    }
    oof[te] <- pk
  }

  list(AUC = as.numeric(pROC::auc(y_all, oof)),
       Brier = mean((y_all - oof)^2))
}

# Baseline with all predictors
base_met <- oof_eval_en(keep_idx = seq_along(all_cols), alpha_val = 0.5)

# One-group-out ablations
abl_rows <- imap_dfr(grp_idx, function(idx_remove, gname){
  keep <- setdiff(seq_along(all_cols), idx_remove)
  met  <- oof_eval_en(keep_idx = keep, alpha_val = 0.5)
  tibble(
    Feature_Removed = gname,
    Removed_n_vars  = length(idx_remove),
    Kept_n_vars     = length(keep),
    AUC             = round(met$AUC, 3),
    dAUC            = round(met$AUC - base_met$AUC, 3),
    Brier           = round(met$Brier, 3),
    dBrier          = round(met$Brier - base_met$Brier, 3)
  )
})

abl_table <- bind_rows(
  tibble(
    Feature_Removed = "None (Full model)",
    Removed_n_vars  = 0L,
    Kept_n_vars     = length(all_cols),
    AUC             = round(base_met$AUC, 3),
    dAUC            = 0,
    Brier           = round(base_met$Brier, 3),
    dBrier          = 0
  ),
  abl_rows
) %>% arrange(desc(Feature_Removed == "None (Full model)"), dAUC)

gt(abl_table) %>%
  tab_header(title = md("**Ablation — Elastic-Net Logistic (OOF, α = 0.5)**")) %>%
  fmt_number(columns = c(AUC, dAUC, Brier, dBrier), decimals = 3)
```



## . Frozen pipeline + holdout scoring


```{r}
# === Save frozen objects after final training on full train ===
final_pipeline <- list(
  # store formula and factor levels if needed
  transform = function(df){
    df %>%
      mutate(
        log_MonthlyCharges = log1p(MonthlyCharges),
        log_TotalCharges   = log1p(TotalCharges),
        charge_per_month   = ifelse(tenure > 0, TotalCharges/tenure, 0),
        tenure_band = cut(tenure, breaks=c(0,6,12,24,48,72),
                          labels=c("0-6","7-12","13-24","25-48","49-72"), include.lowest=TRUE)
      )
  },
  alpha = 0.5,
  lambda = lambda_final,     # from your tuned elastic net
  glmnet_fit = final_fit,    # fitted on full training design matrix
  platt_fit  = platt_mod     # fitted on OOF scores
)
saveRDS(final_pipeline, "final_pipeline.rds")

library(readr)
library(dplyr)

pipe_path <- "final_pipeline.rds"
holdout_path <- "holdout_features.csv"

if (file.exists(pipe_path) && file.exists(holdout_path)) {

  pipe <- readRDS(pipe_path)
  hold <- read_csv(holdout_path)

  hold2 <- pipe$transform(hold)
  Xh <- model.matrix(~ . - 1, data = hold2 %>% select(-ID))
  raw <- as.numeric(predict(pipe$glmnet_fit, Xh, type = "response"))
  ph <- as.numeric(predict(pipe$platt_fit,
                           newdata = data.frame(oof = raw),
                           type = "response"))

  pred <- tibble(customer_id = hold$ID, p_churn = ph)
  write_csv(pred, "predictions.csv")
  message("✅ Holdout scoring completed. predictions.csv saved.")

} else {
  message("⚠️ Holdout file not found — skipping scoring. 
          This cell will activate once holdout_features.csv is released.")
}
```




## . Budget-aware decision rule + cost table per 1,000



- What Our Budget Table Shows

We decided to contact 200 customers per 1,000—about 20% of our base—based on their predicted churn probabilities.
At that level:
	•	Each outreach costs $10, bringing our total outreach cost to $2,000 per 1,000 customers.
	•	We expect to retain around 36 customers out of every 1,000, thanks to targeted intervention.
	•	Those retained customers are projected to generate about $35,493 in future revenue.
	•	After subtracting the outreach cost, we achieve a net gain of roughly $33,500 per 1,000 customers.
	•	The contact threshold is around p ≥ 0.53, meaning customers with a churn probability above 53% are worth contacting.

In simple terms, contacting the top 20% of high-risk customers provides a strong positive return. We keep valuable customers longer while generating an additional $33k per thousand customers compared to doing nothing.


- What Our Capacity Sensitivity Results Tell Us

We tested what happens if we increase our outreach volume:
	•	As we expand outreach from 0 to 400 customers per 1,000, our net retained value continues to increase, though at a slower rate.
	•	At 400 contacts per 1,000, our net gain peaks at about $46k, with total retained value of around $50k and a cost of $4k.
	•	This means that even at our highest capacity, the program remains profitable—each additional contact still adds value, just slightly less than the previous ones.


- How We Interpret These Results

“Our analysis shows that targeting the top 20%–40% of customers by churn probability (p ≥ 0.5) yields between $33k and $46k in net retained revenue per 1,000 customers, even after accounting for outreach costs.
The returns stay positive as we increase outreach, but the marginal gains level off after about 300–400 contacts.
This suggests that our retention budget is well spent at moderate contact levels, maximizing profit while preventing unnecessary spending.”

```{r}
# ============================================================
# Budget-aware decision rule + cost table per 1,000
# Inputs required in env:
#   test_enh        : data.frame with at least Churn, MonthlyCharges
#   prob_test_cal   : numeric calibrated churn probabilities (length = nrow(test_enh))
# Tunables:
#   contacts_per_1000 : outreach capacity
#   cost_per_contact  : outreach unit cost
#   uplift            : P(save | contact, churner)   (treatment uplift)
#   horizon_months    : months of retained revenue credited
#   discount_rate     : optional monthly discount rate for NPV (0 = off)
# ============================================================

suppressPackageStartupMessages({
  library(dplyr); library(gt); library(scales); library(rlang)
})

# ----- 0) Bind data and guards -----
stopifnot(exists("test_enh"), exists("prob_test_cal"))
df <- test_enh %>%
  mutate(
    y = as.integer(Churn == "Yes"),
    p = as.numeric(prob_test_cal)
  )
stopifnot(nrow(df) == length(prob_test_cal))

# ----- 1) Business knobs (edit as needed) -----
contacts_per_1000 <- 200   # capacity per 1,000 customers
cost_per_contact  <- 10    # currency units per contact
uplift            <- 0.25  # chance to prevent churn if contacted and would churn
horizon_months    <- 12    # months of retained revenue credited
discount_rate     <- 0.00  # monthly discount (e.g., 0.01 = 1%/mo)

# Optional revenue model: ARPU per customer = MonthlyCharges
# You can adjust per plan/tenure if desired (examples commented):
# df <- df %>% mutate(
#   ARPU = MonthlyCharges * ifelse(Contract %in% c("One year","Two year"), 1.05, 1.00),
#   H    = ifelse(Contract == "Two year", pmin(horizon_months + 6, 24), horizon_months)
# )
# For now, keep simple:
df <- df %>% mutate(ARPU = MonthlyCharges, H = horizon_months)

# Present value factor for a level stream of ARPU over H months at discount_rate
pv_factor <- function(H, r) {
  ifelse(r <= 0, H, (1 - (1 + r)^(-H)) / r)
}

df <- df %>%
  mutate(
    pv_months = pv_factor(H, discount_rate),     # PV months multiplier
    ev_save   = uplift * p * (ARPU * pv_months)  # expected retained revenue if contacted
  )

# ----- 2) Pick who to contact given capacity -----
N     <- nrow(df)
k_abs <- round(contacts_per_1000 * N / 1000)
ord   <- order(df$p, decreasing = TRUE)
contact_ix <- ord[seq_len(k_abs)]

# Threshold is the minimum probability among contacted
threshold <- min(df$p[contact_ix])

# ----- 3) Aggregate expected outcomes per 1,000 -----
expected_saves_abs <- sum(uplift * df$p[contact_ix])      # expected # of churns prevented in this slice
expected_rev_abs   <- sum(df$ev_save[contact_ix])         # expected retained revenue in currency units
cost_abs           <- k_abs * cost_per_contact            # outreach cost

# Scale to per-1000 basis
scale_1000 <- 1000 / N
cost_tbl <- tibble::tibble(
  per_1000_contacts        = contacts_per_1000,
  per_1000_expected_saves  = expected_saves_abs * scale_1000,
  per_1000_retained_value  = expected_rev_abs   * scale_1000,
  per_1000_outreach_cost   = contacts_per_1000 * cost_per_contact,
  per_1000_net_value       = per_1000_retained_value - per_1000_outreach_cost,
  threshold_used           = threshold,
  uplift_used              = uplift,
  horizon_months_used      = horizon_months,
  discount_rate_used       = discount_rate
)

gt(cost_tbl) |>
  fmt_number(columns = c(per_1000_expected_saves), decimals = 2) |>
  fmt_currency(columns = c(per_1000_retained_value, per_1000_outreach_cost, per_1000_net_value),
               currency = "USD", decimals = 0) |>
  fmt_number(columns = c(threshold_used, uplift_used, discount_rate_used), decimals = 3) |>
  tab_header(title = md("**Budget-aware Decision: Per-1,000 Impact**"),
             subtitle = md(paste0(
               "Top-K by calibrated churn probability. K = ", contacts_per_1000,
               " per 1,000; Cost/contact = $", cost_per_contact,
               "; Uplift = ", uplift, "; Horizon = ", horizon_months, " mo; r = ", discount_rate
             )))

# ----- 4) Optional: capacity curve to justify threshold -----
# Shows per-1,000 net value as you vary capacity from 0..400 contacts per 1,000
caps <- seq(0, 400, by = 20)
cap_curve <- lapply(caps, function(K){
  k <- round(K * N / 1000)
  idx <- ord[seq_len(max(k, 0))]
  saves <- sum(uplift * df$p[idx])
  rev   <- sum(df$ev_save[idx])
  tibble::tibble(
    per_1000_contacts = K,
    per_1000_net_value = (rev * scale_1000) - (K * cost_per_contact),
    per_1000_retained_value = rev * scale_1000,
    per_1000_outreach_cost  = K * cost_per_contact
  )
}) %>% bind_rows()

# Print a small table and the capacity that maximizes net value
cap_best <- cap_curve %>% slice_max(per_1000_net_value, n = 1, with_ties = FALSE)

cap_curve %>%
  arrange(per_1000_contacts) %>%
  gt() |>
  fmt_currency(columns = c(per_1000_retained_value, per_1000_outreach_cost, per_1000_net_value),
               currency = "USD", decimals = 0) |>
  tab_header(title = md("**Capacity Sensitivity (per 1,000)**"),
             subtitle = "Net value across contact limits")

cap_best
```





# . Created and saved OOF predictions (5-fold CV)



```{r}
# ==== OOF predictions — Elastic Net Logistic (α = 0.5) ======================
suppressPackageStartupMessages({
  library(glmnet); library(caret); library(tibble); library(dplyr); library(readr)
})

set.seed(1234)

# --- 0) Inputs check ---------------------------------------------------------
stopifnot(exists("X_train"), exists("y_train"))
X_train <- as.matrix(X_train)
stopifnot(is.numeric(y_train))
n <- nrow(X_train)
stopifnot(length(y_train) == n)

# --- 1) Create 5-fold stratified folds ---------------------------------------
folds  <- caret::createFolds(factor(y_train), k = 5, list = TRUE, returnTrain = FALSE)
foldid <- integer(n)
for (i in seq_along(folds)) foldid[folds[[i]]] <- i
row_id <- seq_len(n)

# --- 2) Generate OOF predictions ---------------------------------------------
alpha_final <- 0.5
oof_log <- rep(NA_real_, n)

for (k in 1:5) {
  te <- which(foldid == k)
  tr <- setdiff(row_id, te)

  cvfit <- cv.glmnet(
    x = X_train[tr, , drop = FALSE],
    y = y_train[tr],
    family = "binomial",
    alpha = alpha_final,
    nfolds = 5,
    type.measure = "auc",
    standardize = TRUE
  )

  fit <- glmnet(
    x = X_train[tr, , drop = FALSE],
    y = y_train[tr],
    family = "binomial",
    alpha = alpha_final,
    lambda = cvfit$lambda.min,
    standardize = TRUE
  )

  oof_log[te] <- as.numeric(predict(fit, X_train[te, , drop = FALSE], type = "response"))
}

stopifnot(!any(is.na(oof_log)))

# --- 3) Save OOF predictions -------------------------------------------------
oof_df <- tibble(
  row_id = row_id,
  model = "logistic_en",
  oof_prob = oof_log,
  y = y_train
)

dir.create("outputs", showWarnings = FALSE)
write_csv(oof_df, "outputs/oof_predictions.csv")

message("OOF predictions saved to outputs/oof_predictions.csv")
# ============================================================================ 
```


# . Final Model Selection

Chosen Model: Elastic Net Logistic Regression (α = 0.50) with Platt Calibration

Alpha ----- 0.5
Lambda----- 0.00799675
AUC_raw -----0.865
AUC_cal ---- 0.865
Brier -----0.129 
Cal_Intercept-----0.008
Cal_Slope -------1.059



Summary: 


The final selected model is the Elastic Net Logistic Regression with Platt Calibration, chosen for its excellent balance between predictive accuracy, calibration, and interpretability. It achieved a calibrated AUC of 0.865 and a Brier score of 0.129, outperforming both Random Forest and XGBoost in reliability and consistency across folds. The calibration intercept (≈ 0.008) and slope (≈ 1.059) indicate nearly perfect probability alignment, confirming that predicted churn probabilities are well-calibrated.

Compared to tree-based models, this approach offers simpler interpretation of feature effects, less variance across folds, and a fully frozen, reproducible scoring pipeline—making it both technically sound and business-friendly for deployment.



# Managerial Recommendation

Our analysis shows that the Elastic Net Logistic Regression with Platt Calibration provides the most reliable and interpretable prediction of customer churn.
With a calibrated AUC of 0.865 and Brier score of 0.129, the model accurately identifies high-risk customers while maintaining well-aligned probability estimates (calibration slope ≈ 1.06).

From a business standpoint, this model enables data-driven retention targeting. By contacting the top 20 % of customers ranked by churn probability (≥ 0.53 threshold), we can expect to retain about 36 customers per 1,000 and generate roughly $33K–$46K in net retained revenue per 1,000 customers after outreach costs.

Recommendations for management:
	•	Deploy the calibrated logistic model monthly to score all active customers.
	•	Focus outreach on the top 20–40 % of high-risk customers for maximum ROI.
	•	Use the probability score as a threshold-based rule that aligns with budget and call-center capacity.
	•	Continue monitoring model performance quarterly to ensure stability as customer behavior and market conditions evolve.

In summary, this model offers a balance of accuracy, simplicity, and operational practicality—allowing the retention team to act confidently on measurable churn probabilities while optimizing cost efficiency.





# . Appendix — AI-Use, Prompts, and Reflection

A1. AI-Use Statement

	•	Tool: ChatGPT (GPT-5 Thinking), OpenAI.
	•	Date of use: November 10, 2025.
	•	Purpose: Structure and tighten the write-up, draft leakage policy language, generate clean R code for OOF predictions, Platt calibration, ablation, and budget-aware cost tables.
	•	Scope of assistance: Text editing for clarity; code scaffolding consistent with my data schema; suggested diagnostics (AUC, Brier, calibration intercept/slope) and file outputs (outputs/oof_predictions.csv, figures/calibration.png).
	•	Human control: I supplied dataset context and constraints, executed and validated all code locally, selected final hyperparameters, and made all modeling decisions.
	•	Attribution: Portions of the prose and code structure were AI-assisted; all results were reproduced and verified by me.

A2. Representative Prompts Used

	•	“Write a leakage policy paragraph for a churn model scored monthly. Keep it concise and concrete.”
	•	“Give R code to compute 5-fold OOF probabilities for elastic-net logistic (alpha = 0.5) with per-fold lambda via cv.glmnet, and save a CSV.”
	•	“Produce a calibration plot from OOF predictions and save to figures/calibration.png using ggplot2.”
	•	“Create a budget-aware decision table per 1,000 customers using calibrated probabilities, cost per contact, uplift, and a 12-month horizon.”
	•	“Draft a one-paragraph final-model selection note that reports Test AUC, Brier, and calibration intercept/slope and justifies elastic net over RF/XGB.”

A3. Verification and Safeguards
	
	•	Reproducibility: Re-ran the notebook from a clean session; confirmed creation of outputs/oof_predictions.csv and figures/calibration.png.
	•	No leakage: Ensured all encodings came from train only; OOF predictions used strict fold separation; Platt model fit on OOF only.
	•	Metric checks: Recomputed AUC, Brier, calibration intercept/slope on test; inspected ROC and reliability curves.
	•	File hygiene: All paths relative; figures and outputs created by code.

A4. Reflection
	
	•	Value: Accelerated drafting and reduced boilerplate for calibration and OOF scaffolding.
	•	Limitations: Generated code required alignment with my column names and factor levels; model choices still depended on domain constraints and validation.
	•	Net effect: Saved time on syntax and formatting while keeping modeling decisions, tuning, and validation under my control.
